from snakemake.utils import min_version
from pandas import read_csv
from os.path import join, isfile

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("7")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0)


# IMPORT COMMON FUNCTIONS:
include: "rules/common.py"

# SET GLOBAL DOCKER RECIPIE TO USE:
container: "docker://graemeford/pipeline-os"

# group: VALIDATE
rule strip_INFO_tags:
    """
    Strip out INFO tags
    """
    group: "VALIDATE"
    input:
        lambda wildcards: datasets.loc[
            datasets["dataset_name"] == wildcards.dataset, "file"
        ].item(),
    output:
        "results/PREP/{dataset}/strip_INFO_tags.vcf.gz"
    log:
        "_logs/PREP/{dataset}/strip_INFO_tags.log"
    benchmark:
        "_benchmarks/VALIDATE/strip_INFO_tags/{dataset}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        bcftools annotate -x INFO,FORMAT -Oz -o {output} {input} 2>{log}
        """

# group: VALIDATE
# TODO: Identify Replacement for Piccards `FixVcfHeader`.
# rule FixVcfHeader:
#     """
#     Validate the VCF header and specification has been followed
#     """
#     group: "VALIDATE"
#     input:
#         "results/PREP/{dataset}/strip_INFO_tags.vcf.gz"
#     output:
#         "results/PREP/{dataset}/FixVcfHeader.vcf.gz"
#     benchmark:
#         "results/_benchmarks/VALIDATE/FixVcfHeader/{dataset}.tsv"
#     resources:
#         cpus=search("cores", "VALIDATE"),
#         nodes=search("nodes", "VALIDATE"),
#         queue=search("queue", "VALIDATE"),
#         walltime=search("walltime", "VALIDATE")
#     envmodules:
#         config["environment"]["envmodules"]["piccard"],
#     params:
#         memory=search("memory", "VALIDATE")
#     shell:
#         """
#         echo -e "--- LOG SECTION START | GATK 'FixVcfHeader' ---" 1>&2
#         PicardCommandLine -Xmx{params.memory} FixVcfHeader I={input} O={output}
#         echo -e "--- LOG SECTION END | GATK 'FixVcfHeader' ---\n" 1>&2
#         """

# group: VALIDATE
rule filter:
    """
    Remove variant types we cant yet analyse
    """
    group: "VALIDATE"
    input:
        "results/PREP/{dataset}/strip_INFO_tags.vcf.gz"
    output:
        "results/PREP/{dataset}/filter.vcf.gz"
        # pipe("results/PREP/{dataset}/filter.vcf.gz")
    log:
        "_logs/PREP/{dataset}/filter.log"
    benchmark:
        "_benchmarks/VALIDATE/filter/{dataset}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        memory=search("memory", "VALIDATE")
    shell:
        """
        bcftools view -v snps,indels -f PASS -O z -o {output} < {input} 2>{log}
        """

# group: VALIDATE
# conditional: true
rule subset:
    """
    Subset samples according to user defined list and remove variants that do not pass QC.
    """
    group: "VALIDATE"
    input:
        vcf="results/PREP/{dataset}/filter.vcf.gz",
        samples="input/samples.csv"
    output:
        "results/PREP/{dataset}/subset.vcf.gz"
    log:
        "_logs/PREP/{dataset}/subset.log"
    benchmark:
        "_benchmarks/VALIDATE/subset/{dataset}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        samples=lambda wildcards, input: ",".join(read_csv(join("input", "samples.csv"), header=0).query("dataset == 'GnomAD'")["sample_name"].tolist())
    shell:
        """
        bcftools view -s {params.samples} -O z -o {output} {input.vcf}  2>{log}
        """
 
# group: VALIDATE
rule normalize:
    """
    Normalise variants - split multiallelic records, left align variants, and ensure variant parsimony
    """
    group: "VALIDATE"
    input:
        lambda wildcards: "results/PREP/{dataset}/subset.vcf.gz" if isfile("input/samples.csv") else "results/PREP/{dataset}/filter.vcf.gz"
    output:
        "results/PREP/{dataset}/normalize.vcf.gz"
        # pipe("results/PREP/{dataset}/normalize.vcf.gz")
    log:
        "_logs/PREP/{dataset}/normalize.log"
    benchmark:
        "results/_benchmarks/VALIDATE/normalize/{dataset}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        bcftools norm -m -any -O z -o {output} < {input} 2>{log}
        """

# group: VALIDATE
rule annotate:
    """
    Ensure consistency of chromosome nomenclature for each dataset and sort by chromosome
    """
    group: "VALIDATE"
    input:
        "results/PREP/{dataset}/normalize.vcf.gz"
    output:
        "results/PREP/{dataset}/annotate.vcf.gz"
    log:
        "_logs/PREP/{dataset}/annotate.log"
    benchmark:
        "_benchmarks/VALIDATE/annotate/{dataset}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        memory=search("memory", "VALIDATE")
    shell:
        # TODO: Break up this internal piping and use modular rules. Especially with sort operations which have different memory requirements, worthy of consideration in a data streaming environment.
        """
        bcftools annotate --rename-chrs input/rename_chr.txt {input} | bcftools sort -m {params.memory} -T results/PREP/{wildcards.dataset} -O z -o {output} 2>{log}
        """

# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    group: "VALIDATE"
    input:
        "results/{operation}/{dataset}/{output}.vcf.gz"
    output:
        "results/{operation}/{dataset}/{output}.vcf.gz.tbi"
    log:
        "_logs/{operation}/{dataset}/{output}.log"
    benchmark:
        "_benchmarks/{operation}/{dataset}/{output}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        tabix -p vcf {input}
        """

# TODO: IF (NEEDS LIFTOVER)
rule liftover:
    group: "LIFTOVER"
    input:
        "results/PREP/{dataset}/annotate.vcf.gz",
        "results/PREP/{dataset}/annotate.vcf.gz.tbi",
    output:
        "results/PREP/{dataset}/liftover.vcf.gz",
        "results/PREP/{dataset}/liftover_rejected.vcf.gz"
    log:
        "_logs/PREP/{dataset}/liftover.log"
    benchmark:
        "_benchmarks/PREP/{dataset}/liftover.benchmark"
    envmodules:
        config["environment"]["envmodules"]["piccard"]
    params:
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        ref=join(
            *next(
                reference_genome["file_path"]
                for reference_genome in config["reference-genomes"]
                if reference_genome["version"] == "GRCh38"
            ),
        ),
    shell:
        """
        PicardCommandLine LiftoverVcf I={input[0]} O={output[0]} R={params.ref} C={params.chainFile} REJECT={output[1]} 2>{log}
        """

STart conditional output file list ocnstruction:
output_files = list()

output_files.append(expand("results/PREP/{dataset}/annotate.vcf.gz", dataset=list(datasets["dataset_name"])))


for dataset in samples["dataset"]:
    if dataset != "GRCh38":
        output_files.append(
            f"results/PREP/{dataset}/liftover.vcf.gz"
        )

rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        output_files