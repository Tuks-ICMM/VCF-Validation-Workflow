from snakemake.utils import min_version
from pandas import read_csv, MultiIndex
from os.path import join, isfile

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("7")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "manifest.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0, dtype=dict(dataset=str, reference_genome=str, contig=str, file=str))


# IMPORT COMMON FUNCTIONS:
include: "rules/common.smk"

# SET GLOBAL DOCKER RECIPIE TO USE:
container: "docker://graemeford/pipeline-os"




rule tabix:
    log: outputDir("tmp/{contig}/{dataset}/{operation}_tabix.log")
    benchmark: outputDir("tmp/{contig}/{dataset}/{operation}_tabix.benchmark")
    wildcard_constraints:
        contig=r"[0-9]{1,2}", # TODO: Make this configurable
        dataset=r"[a-zA-Z0-9\-]+",
        operation=r"[a-zA-Z0-9\-\_]+",
    input:
        outputDir("tmp/{contig}/{dataset}/{operation}.vcf.gz")
    output:
        outputDir("tmp/{contig}/{dataset}/{operation}.vcf.gz.tbi")
    shell:
        """
        tabix -p vcf {input}
        """

rule clear_annotations:
    log: outputDir("tmp/{contig}/{dataset}/cleared_annotations.log")
    benchmark: outputDir("tmp/{contig}/{dataset}/cleared_annotations.benchmark")
    wildcard_constraints:
        contig=r"[0-9]{1,2}", # TODO: Make this configurable
        dataset=r"[a-zA-Z0-9\-]+"
    input:
        vcf=lambda wildcards: datasets.loc[
            (datasets["dataset_name"] == wildcards.dataset) &
             (datasets["contig"] == wildcards.contig), "file"
        ].item(),
    output:
        vcf=outputDir("tmp/{contig}/{dataset}/cleared_annotations.vcf.gz")
    shell:
        """
        bcftools annotate -x INFO,FORMAT -Oz -o {output.vcf} {input.vcf} 2>{log}
        """


rule normalize_variant_records:
    log: outputDir("tmp/{contig}/{dataset}/normalized_variant_records.log")
    benchmark: outputDir("tmp/{contig}/{dataset}/normalized_variant_records.benchmark")
    wildcard_constraints:
        contig=r"[0-9]{1,2}", # TODO: Make this configurable
        dataset=r"[a-zA-Z0-9\-]+"
    input:
        vcf=outputDir("tmp/{contig}/{dataset}/cleared_annotations.vcf.gz"),
        tbi=outputDir("tmp/{contig}/{dataset}/cleared_annotations.vcf.gz.tbi"),
    output:
        vcf=outputDir("tmp/{contig}/{dataset}/normalized_variant_records.vcf.gz")
    shell:
        """
        bcftools norm -m -any -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


rule filter_variant_types:
    log: outputDir("tmp/{contig}/{dataset}/filtered_variant_types.log")
    benchmark: outputDir("tmp/{contig}/{dataset}/filtered_variant_types.benchmark")
    wildcard_constraints:
        contig=r"[0-9]{1,2}", # TODO: Make this configurable
        dataset=r"[a-zA-Z0-9\-]+"
    input:
        vcf=outputDir("tmp/{contig}/{dataset}/normalized_variant_records.vcf.gz"),
        tbi=outputDir("tmp/{contig}/{dataset}/normalized_variant_records.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/{contig}/{dataset}/filtered_variant_types.vcf.gz")
    shell:
        """
        bcftools view -v snps -f PASS -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


rule sort_variant_records:
    log: outputDir("tmp/{contig}/{dataset}/sorted_variant_records.log")
    benchmark: outputDir("tmp/{contig}/{dataset}/sorted_variant_records.benchmark")
    wildcard_constraints:
        contig=r"[0-9]{1,2}", # TODO: Make this configurable
        dataset=r"[a-zA-Z0-9\-]+"
    params:
        tmp=lambda wildcards,output: output["vcf"][:-7] # TODO: Convert to more scalable method using string `split()` or similar approach
    input:
        vcf=outputDir("tmp/{contig}/{dataset}/filtered_variant_types.vcf.gz"),
        tbi=outputDir("tmp/{contig}/{dataset}/filtered_variant_types.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/{contig}/{dataset}/sorted_variant_records.vcf.gz")
    shell:
        """
        bcftools sort -T {params.tmp} -O z -o {output.vcf} < {input.vcf} 2>{log}
        """



# Start conditional output file list ocnstruction:
output_files = list()

output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/sorted_variant_records.vcf.gz"), 
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/sorted_variant_records.vcf.gz.tbi"), 
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)

# Include output for rule 'normalized_variant_records'
output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/normalized_variant_records.vcf.gz"), 
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/normalized_variant_records.vcf.gz.tbi"),
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)


# Include output for rule 'filtered_variant_types'
output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/filtered_variant_types.vcf.gz"), 
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/{contig}/{dataset}/filtered_variant_types.vcf.gz.tbi"), 
        dataset=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)


# Here, we use a `Pandas` `MultiIndex` to group our datasets by ["reference_genome", "dataset_name"]. This allows
# us to iterate over both the `reference_genome` columns unique values AND the `dataset_name` columns and access
# their unique values.
#
# > Think of `MultiIndexes` as an Excel Sheet with the header row and the index columns (Can have more than one)
# > are fixed. Now take that, but add a Z-dimension to the data in the middle that can stills croll around. It 
# > now has levels and you go up or down however many levels there are and you get a whole new set of values for 
# > the colunms and row indexes.
# > 
# > This way of working with data helps us select our data in an intuitive way that also filters it by nature, 
# > since to select the data, you simply need to be able to describe what data you want. This is better for code 
# > compatability as it relies less heavily on query method implementation, and more on data science principles.
#
# TODO: Evaluate porting metadata to MultiIndexes where appropriate.
for reference_genome, groupby_subset in datasets.set_index(["reference_genome", "file"]).groupby(level=0): # [FOR] all unqiue (dataset_name and reference_genome) column combinations present
    if reference_genome != "GRCh38" and groupby_subset is not None: # [IF] reference genome version
        for dataset_name in groupby_subset.index.get_level_values(0): # [FOR] the column in our MultiIndex that contains the dataset_name's in this subset
            output_files.extend(
                # [EACH] add liftover request for the DAG
                outputDir("PREP/{dataset}/liftover.vcf.gz").format(dataset_name)
            )
            output_files.extend(
                # [EACH]{QC} add liftover snapshot
                outputDir("PREP/{dataset}/liftover.pdf").format(dataset_name)
            )
rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log: outputDir("all.log"),
    benchmark: outputDir("all.benchmark")
    input:
        # [item for sublist in output_files for item in sublist],
        output_files