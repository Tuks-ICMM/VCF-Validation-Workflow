from snakemake.utils import min_version
from pandas import read_csv, MultiIndex
from os.path import join, isfile

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("7")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0, dtype=dict(dataset_name=str, reference_genome=str, contig=str, file=str))


# IMPORT COMMON FUNCTIONS:
include: "rules/common.smk"

# SET GLOBAL DOCKER RECIPIE TO USE:
container: "docker://graemeford/pipeline-os"


rule clear_annotations:
    """
    Strip out INFO tags
    """
    log: outputDir("tmp/cleared_annotations/{contig}/{dataset_name}_cleared_annotations.log")
    benchmark: outputDir("tmp/cleared_annotations/{contig}/{dataset_name}_cleared_annotations.benchmark")
    input:
        vcf=lambda wildcards: datasets.loc[
            (datasets["dataset_name"] == wildcards.dataset_name) &
             (datasets["contig"] == wildcards.contig), "file"
        ].item(),
    output:
        vcf=outputDir("tmp/cleared_annotations/{contig}/{dataset_name}_cleared_annotations.vcf.gz")
    shell:
        """
        bcftools annotate -x INFO,FORMAT -Oz -o {output.vcf} {input.vcf} 2>{log}
        """


rule normalize_variant_records:
    """
    Normalise variants - split multiallelic records, left align variants, and ensure variant parsimony
    """
    log: outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.log")
    benchmark: outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.benchmark")
    input:
        vcf=outputDir("tmp/cleared_annotations/{contig}/{dataset_name}_cleared_annotations.vcf.gz"),
        tbi=outputDir("tmp/cleared_annotations/{contig}/{dataset_name}_cleared_annotations.vcf.gz.tbi"),
    output:
        vcf=outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.vcf.gz")
    shell:
        """
        bcftools norm -m -any -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


rule filter_variant_types:
    """
    Remove variant types we cant yet analyse
    """
    log: outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.log")
    benchmark: outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.benchmark")
    input:
        vcf=outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.vcf.gz"),
        tbi=outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.vcf.gz")
    shell:
        """
        bcftools view -v snps -f PASS -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


rule sort_variant_records:
    """
    Ensure consistency of chromosome nomenclature for each dataset and sorted_variant_records by chromosome
    """
    log: outputDir("tmp/sorted_variant_records/{contig}/{dataset_name}_sorted_variant_records.log")
    benchmark: outputDir("tmp/sorted_variant_records/{contig}/{dataset_name}_sorted_variant_records.benchmark")
    params:
        tmp=lambda wildcards,output: output["vcf"][:-7] # TODO: Convert to more scalable method using string `split()` or similar approach
    input:
        vcf=outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.vcf.gz"),
        tbi=outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/sorted_variant_records/{contig}/{dataset_name}_sorted_variant_records.vcf.gz")
    shell:
        """
        bcftools sort -T {params.tmp} -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


rule liftover:
    """
    This rule helps liftover data from one reference genome to another.
    """
    log: outputDir("tmp/liftover/{contig}/{dataset_name}_liftover.log")
    benchmark: outputDir("tmp/liftover/{contig}/{dataset_name}_liftover.benchmark")
    params:
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        ref=join(
            *next(
                reference_genome["file_path"]
                for reference_genome in config["reference-genomes"]
                if reference_genome["version"] == "GRCh38"
            ),
        ),
    input:
        vcf=outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_sorted_variant_records.vcf.gz"),
        tbi=outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_sorted_variant_records.vcf.gz.tbi"),
    output:
        vcf=outputDir("tmp/liftover/{contig}/{dataset_name}_liftover.vcf.gz"),
        rejected=outputDir("tmp/liftover/{contig}/{dataset_name}_liftover_rejected.vcf.gz")
    shell:
        """
        java -jar $PICARD LiftoverVcf I={input.vcf} O={output.vcf} R={params.ref} C={params.chainFile} REJECT={output.rejected} 2>{log}
        """


rule tabix:
    """
    Generate tabix-index.
    """
    log: outputDir("tmp/{operation}/{contig}/{dataset_name}_tabix.log")
    benchmark: outputDir("tmp/{operation}/{contig}/{dataset_name}_tabix.benchmark")
    input:
        outputDir("tmp/{operation}/{contig}/{dataset_name}_{operation}.vcf.gz")
    output:
        outputDir("tmp/{operation}/{contig}/{dataset_name}_{operation}.vcf.gz.tbi")
    shell:
        """
        tabix -p vcf {input}
        """


# Start conditional output file list ocnstruction:
output_files = list()

output_files.extend(
    expand(
        outputDir("tmp/sorted_variant_records/{contig}/{dataset_name}_sorted_variant_records.vcf.gz"), 
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/sorted_variant_records/{contig}/{dataset_name}_sorted_variant_records.vcf.gz.tbi"), 
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)

# Include output for rule 'normalized_variant_records'
output_files.extend(
    expand(
        outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.vcf.gz"), 
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/normalized_variant_records/{contig}/{dataset_name}_normalized_variant_records.vcf.gz.tbi"),
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)


# Include output for rule 'filtered_variant_types'
output_files.extend(
    expand(
        outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.vcf.gz"), 
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)
output_files.extend(
    expand(
        outputDir("tmp/filtered_variant_types/{contig}/{dataset_name}_filtered_variant_types.vcf.gz.tbi"), 
        dataset_name=datasets["dataset_name"].unique().tolist(),
        contig=datasets["contig"].unique().tolist()
        )
)


# Here, we use a `Pandas` `MultiIndex` to group our datasets by ["reference_genome", "dataset_name"]. This allows
# us to iterate over both the `reference_genome` columns unique values AND the `dataset_name` columns and access
# their unique values.
#
# > Think of `MultiIndexes` as an Excel Sheet with the header row and the index columns (Can have more than one)
# > are fixed. Now take that, but add a Z-dimension to the data in the middle that can stills croll around. It 
# > now has levels and you go up or down however many levels there are and you get a whole new set of values for 
# > the colunms and row indexes.
# > 
# > This way of working with data helps us select our data in an intuitive way that also filters it by nature, 
# > since to select the data, you simply need to be able to describe what data you want. This is better for code 
# > compatability as it relies less heavily on query method implementation, and more on data science principles.
#
# TODO: Evaluate porting metadata to MultiIndexes where appropriate.
for reference_genome, groupby_subset in datasets.set_index(["reference_genome", "file"]).groupby(level=0): # [FOR] all unqiue (dataset_name and reference_genome) column combinations present
    if reference_genome != "GRCh38" and groupby_subset is not None: # [IF] reference genome version
        for dataset_name in groupby_subset.index.get_level_values(0): # [FOR] the column in our MultiIndex that contains the dataset_name's in this subset
            output_files.extend(
                # [EACH] add liftover request for the DAG
                outputDir("PREP/{dataset_name}/liftover.vcf.gz").format(dataset_name)
            )
            output_files.extend(
                # [EACH]{QC} add liftover snapshot
                outputDir("PREP/{dataset_name}/liftover.pdf").format(dataset_name)
            )
rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log: outputDir("all.log"),
    benchmark: outputDir("all.benchmark")
    input:
        # [item for sublist in output_files for item in sublist],
        output_files