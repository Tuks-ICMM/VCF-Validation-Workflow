from snakemake.utils import min_version
from pandas import read_csv
from os.path import join

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("6.0")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0)


# IMPORT COMMON FUNCTIONS:
include: "rules/common.py"


rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    log:
        "logs/ALL/ALL.log",
    input:
        expand("results/PREP/{dataset}.vcf.gz", dataset=list(datasets["dataset_name"])),
        expand(
            "results/LIFTOVER/{dataset}.vcf.gz", dataset=list(datasets["dataset_name"])
        ),


rule VALIDATE:
    """
    Perform normalisation of variants, validation of VCF format as well as REF alleles and strip out INFO tags.
    """
    log:
        "results/PREP/{sample}.log",
    input:
        lambda wildcards: datasets.loc[
            datasets["dataset_name"] == wildcards.sample, "file"
        ].item(),
    output:
        "results/PREP/{sample}.vcf.gz",
        "results/PREP/{sample}.vcf.gz.tbi",
    params:
        memory=search("memory", "VALIDATE"),
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE"),
    envmodules:
        "bcftools-1.7",
        "picard-2.17.11",
        "gatk-4.2.2.0",
        "samtools-1.7"
    container:
        "docker://graemeford/pipeline-os"
    shell:
        """
        # Strip out INFO tags:
        echo -e "\nLOG SECTION START | bcftools annotate\n" 2>> {log}
        bcftools annotate -x INFO,FORMAT -O z -o results/PREP/{wildcards.sample}_NO_INFO.vcf.gz {input} 2>> {log}
        echo -e "\nLOG SECTION END | bcftools annotate" 2>> {log}

        # Regenerate and verify the VCF header:
        echo -e "\nLOG START 'GATK FixVcfHeader'\n" 2>> {log}
        java -Xmx{params.memory} -jar $PICARD FixVcfHeader I=results/PREP/{wildcards.sample}_NO_INFO.vcf.gz O=results/PREP/{wildcards.sample}_NEW_INFO.vcf.gz 2>> {log}
        echo -e "\nLOG SECTION END 'GATK FixVcfHeader'\n" 2>> {log}

        # Remove variant types we cant yet analyse:
        echo -e "\nLOG SECTION START 'GATK SelectVariants'\n" 2>> {log}
        gatk SelectVariants -V results/PREP/{wildcards.sample}_NEW_INFO.vcf.gz --select-type-to-include SNP --select-type-to-include INDEL --select-type-to-exclude MIXED --select-type-to-exclude MNP --select-type-to-exclude SYMBOLIC --exclude-filtered -O results/PREP/{wildcards.sample}_FILTERED.vcf.gz 2>> {log}
        echo -e "\nLOG SECTION END 'GATK FixVcfHeader'\n" 2>> {log}
        
        # Subset samples according to user defined list and remove variants that do not pass QC:
        echo -e "\nLOG SECTION START 'bcftools view'\n" 2>> {log}
        bcftools view -S input/{wildcards.sample}_samples_subset.txt -f 'PASS' -O z -o results/PREP/{wildcards.sample}_FILTERED2.vcf.gz results/PREP/{wildcards.sample}_FILTERED.vcf.gz 2>> {log}
        echo -e "\nLOG SECTION END 'bcftools view'\n" 2>> {log}
        
        # Normalise variants - split multiallelic records, left align variants, and ensure variant parsimony
        echo -e "\nLOG SECTION START 'bcftools norm'\n" 2>> {log}
        bcftools norm -m -any results/PREP/{wildcards.sample}_FILTERED2.vcf.gz -O z -o results/PREP/{wildcards.sample}_NORMALISED.vcf.gz 2>> {log}
        echo -e "\nLOG SECTION END 'bcftools norm'\n" 2>> {log}

        # Ensure consistency of chromosome nomenclature for each dataset and sort by chromosome
        echo -e "\nLOG SECTION START 'bcftools annotate'\n" 2>> {log}
        bcftools annotate --rename-chrs input/rename_chr.txt results/PREP/{wildcards.sample}_NORMALISED.vcf.gz | bcftools sort -m {params.memory} -T results/PREP -O z -o results/PREP/{wildcards.sample}.vcf.gz 2>> {log}
        echo -e "\nLOG SECTION END 'bcftools annotate'\n" 2>> {log}

        tabix -p vcf results/PREP/{wildcards.sample}.vcf.gz 2>> {log}
        """


rule LIFTOVER:
    """
    Lift Variants onto same Reference build. Otherwise we cant merge them or analyse them in context of each other.
    """
    log:
        "logs/LIFTOVER/{sample}.log",
    input:
        "results/PREP/{sample}.vcf.gz",
        "results/PREP/{sample}.vcf.gz.tbi",
    output:
        "results/LIFTOVER/{sample}.vcf.gz",
    params:
        prefix=lambda wildcards: "results/LIFTOVER/{sample}_LIFTED".format(
            sample=wildcards.sample
        ),
        exclusionList=(
            lambda wildcards: "results/LIFTOVER/{sample}_EXCLUDE.dat".format(
                sample=wildcards.sample
            )
        ),
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        LiftOver=join("resources", "liftOverPlink.py"),
        rmBadLifts=join("resources", "rmBadLifts.py"),
        ref=join(
            *next(
                i["file_path"]
                for i in config["reference-genomes"]
                if i["version"] == "GRCh38"
            ),
        ),
        mem=f'-Xmx{search("memory", "LIFTOVER")} ',
    resources:
        cpus=search("cores", "LIFTOVER"),
        nodes=search("nodes", "LIFTOVER"),
        queue=search("queue", "LIFTOVER"),
        walltime=search("walltime", "LIFTOVER"),
    container:
        "docker://graemeford/pipeline-os"
    script:
        join("scripts", "00-LIFTOVER.py")
