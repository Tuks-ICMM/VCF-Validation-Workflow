from snakemake.utils import min_version
from pandas import read_csv, MultiIndex
from os.path import join, isfile

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("7")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0)


# IMPORT COMMON FUNCTIONS:
include: "rules/common.smk"

# SET GLOBAL DOCKER RECIPIE TO USE:
container: "docker://graemeford/pipeline-os"

# group: VALIDATE
rule wipeInfo:
    """
    Strip out INFO tags
    """
    log: outputDir("tmp/wipeInfo/{dataset_name}_wipeInfo.log")
    benchmark: outputDir("tmp/wipeInfo/{dataset_name}_wipeInfo.benchmark")
    resources:
        cpus=search("cores", "wipeInfo"),
        nodes=search("nodes", "wipeInfo"),
        queue=search("queue", "wipeInfo"),
        walltime=search("walltime", "wipeInfo")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    input:
        vcf=lambda wildcards: datasets.loc[
            datasets["dataset_name"] == wildcards.dataset_name, "file"
        ].unique().item(),
    output:
        vcf=outputDir("tmp/wipeInfo/{dataset_name}_wipeInfo.vcf.gz")
    shell:
        """
        bcftools annotate -x INFO,FORMAT -Oz -o {output.vcf} {input.vcf} 2>{log}
        """

# group: VALIDATE
rule normalize:
    """
    Normalise variants - split multiallelic records, left align variants, and ensure variant parsimony
    """
    log: outputDir("tmp/normalize/{dataset_name}_normalize.log")
    benchmark: outputDir("tmp/normalize/{dataset_name}_normalize.benchmark")
    resources:
        cpus=search("cores", "normalize"),
        nodes=search("nodes", "normalize"),
        queue=search("queue", "normalize"),
        walltime=search("walltime", "normalize")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    input:
        vcf=outputDir("tmp/wipeInfo/{dataset_name}_wipeInfo.vcf.gz"),
        tbi=outputDir("tmp/wipeInfo/{dataset_name}_wipeInfo.vcf.gz.tbi"),
    output:
        vcf=outputDir("tmp/normalize/{dataset_name}_normalize.vcf.gz")
        # pipe(outputDir("tmp/normalize/{dataset_name}_normalize.vcf.gz"))
    shell:
        """
        bcftools norm -m -any -O z -o {output.vcf} < {input.vcf} 2>{log}
        """


# group: VALIDATE
rule filter:
    """
    Remove variant types we cant yet analyse
    """
    log: outputDir("tmp/filter/{dataset_name}_filter.log")
    benchmark: outputDir("tmp/filter/{dataset_name}_filter.benchmark")
    resources:
        cpus=search("cores", "filter"),
        nodes=search("nodes", "filter"),
        queue=search("queue", "filter"),
        walltime=search("walltime", "filter")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        memory=search("memory", "filter")
    input:
        vcf=outputDir("tmp/sort/{dataset_name}_normalize.vcf.gz"),
        tbi=outputDir("tmp/sort/{dataset_name}_normalize.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/filter/{dataset_name}_filter.vcf.gz")
        # pipe(outputDir("PREP/{dataset_name}/filter.vcf.gz"))
    shell:
        """
        bcftools view -v snps -f PASS -O z -o {output.vcf} < {input.vcf} 2>{log}
        """

# group: VALIDATE
rule sort:
    """
    Ensure consistency of chromosome nomenclature for each dataset and sort by chromosome
    """
    log: outputDir("tmp/sort/{dataset_name}_sort.log")
    benchmark: outputDir("tmp/sort/{dataset_name}_sort.benchmark")
    resources:
        cpus=search("cores", "sort"),
        nodes=search("nodes", "sort"),
        queue=search("queue", "sort"),
        walltime=search("walltime", "sort")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        memory=search("memory", "sort"),
        tmp=lambda wildcards,output: output["vcf"][:-7]
    input:
        vcf=outputDir("tmp/normalize/{dataset_name}_filter.vcf.gz"),
        tbi=outputDir("tmp/normalize/{dataset_name}_filter.vcf.gz.tbi")
    output:
        vcf=outputDir("tmp/sort/{dataset_name}_sort.vcf.gz")
    shell:
        # TODO: Break up this internal piping and use modular rules. Especially with sort operations which have different memory requirements, worthy of consideration in a data streaming environment.
        """
        bcftools sort -m {params.memory} -T {params.tmp} -O z -o {output.vcf} < {input.vcf} 2>{log}
        """
# # group: VALIDATE
# # TODO: Remove this step in favour of E! Ensembl annotations
# rule annotate:
#     """
#     This rule uses a VCF file provided by RefSeq to annotate known variants
#     """
#     log: outputDir("tmp/annotate/{dataset_name}_annotate.log")
#     benchmark: outputDir("tmp/annotate/{dataset_name}_annotate.benchmark")
#     resources:
#         cpus=search("cores", "annotate"),
#         nodes=search("nodes", "annotate"),
#         queue=search("queue", "annotate"),
#         walltime=search("walltime", "annotate")
#     envmodules:
#         config["environment"]["envmodules"]["bcftools"],
#     input: 
#         vcf=outputDir("tmp/filter/{dataset_name}_filter.vcf.gz"),
#         tbi=outputDir("tmp/filter/{dataset_name}_filter.vcf.gz.tbi"),
#         annotations="resources/annotations.vcf.gz"
#     output:
#         vcf=outputDir("tmp/annotate/{dataset_name}_annotate.vcf.gz")
#     shell:
#         # --rename-chrs input/rename_chr.txt would go here
#         """
#         bcftools annotate -c ID -a {input.annotations} -I +'%CHROM:%POS|%REF-%FIRST_ALT' -O z -o {output.vcf} {input.vcf}
#         """

# group: LIFTOVER
rule liftover:
    """
    This rule helps liftover data from one reference genome to another.
    """
    log: outputDir("tmp/liftover/{dataset_name}_liftover.log")
    benchmark: outputDir("tmp/liftover/{dataset_name}_liftover.benchmark")
    resources:
        cpus=search("cores", "liftover"),
        nodes=search("nodes", "liftover"),
        queue=search("queue", "liftover"),
        walltime=search("walltime", "liftover")
    envmodules:
        config["environment"]["envmodules"]["picard"]
    params:
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        ref=join(
            *next(
                reference_genome["file_path"]
                for reference_genome in config["reference-genomes"]
                if reference_genome["version"] == "GRCh38"
            ),
        ),
    input:
        vcf=outputDir("tmp/filter/{dataset_name}_sort.vcf.gz"),
        tbi=outputDir("tmp/filter/{dataset_name}_sort.vcf.gz.tbi"),
    output:
        vcf=outputDir("tmp/liftover/{dataset_name}_liftover.vcf.gz"),
        rejected=outputDir("tmp_liftover/{dataset_name}_liftover_rejected.vcf.gz")
    shell:
        """
        java -jar $PICARD LiftoverVcf I={input.vcf} O={output.vcf} R={params.ref} C={params.chainFile} REJECT={output.rejected} 2>{log}
        """

# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    log: outputDir("tmp/{operation}/{dataset_name}_tabix.log")
    benchmark: outputDir("tmp/{operation}/{dataset_name}_tabix.benchmark")
    resources:
        cpus=search("cores", "tabix"),
        nodes=search("nodes", "tabix"),
        queue=search("queue", "tabix"),
        walltime=search("walltime", "tabix")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    input:
        outputDir("tmp/{operation}/{dataset_name}_{operation}.vcf.gz")
    output:
        outputDir("tmp/{operation}/{dataset_name}_{operation}.vcf.gz.tbi")
    shell:
        """
        tabix -p vcf {input}
        """


# TODO: Potential gist snippet
# rule stats:
#     log:  outputDir("tmp/{operation}/{dataset_name}_bcfstats.log"
#     benchmark:  outputDir("tmp/{operation}/{dataset_name}_bcfstats.benchmark"
#     resources:
#         cpus=search("cores", "stats"),
#         nodes=search("nodes", "stats"),
#         queue=search("queue", "stats"),
#         walltime=search("walltime", "stats")
#     envmodules:
#         config["environment"]["envmodules"]["latex"],
#         config["environment"]["envmodules"]["bcftools"]
#     input: 
#         final = outputDir("tmp/{operation}/{dataset_name}_{operation}.vcf.gz",
#         final_tbi = outputDir("tmp/{operation}/{dataset_name}_{operation}.vcf.gz.tbi",
#         initial = lambda wildcards: getattr(rules, str(wildcards.operation)).input.vcf,
#         initial_tbi = lambda wildcards: getattr(rules, str(wildcards.operation)).input.tbi
#     output: 
#         report(
#             directory(outputDir("tmp/{operation}/"),
#             patterns=["{dataset_name}_{operation}.png"],
#             category="Data processing and validation",
#             subcategory="{dataset_name}",
#             caption="report/stats.rst",
#             labels={
#                 "Dataset": "{dataset_name}",
#                 "Operation": "{operation}",
#             }
#             )
#     shell:
#         """
#         bcftools stats {input.initial} {input.final} > All.vchk
#         plot-vcfstats -p {output} All.vchk
#         """

# Start conditional output file list ocnstruction:
output_files = list()

output_files.append(
    expand(
        outputDir("tmp/sort/{dataset_name}_sort.vcf.gz"), 
        dataset_name=list(datasets["dataset_name"])
        )
)
output_files.append(
    expand(
        outputDir("tmp/sort/{dataset_name}_sort.vcf.gz.tbi"), 
        dataset_name=list(datasets["dataset_name"])
        )
)

# Include output for rule 'annotate'
# output_files.append(
#     expand(
#         outputDir("tmp/annotate/{dataset_name}_annotate.vcf.gz", 
#         dataset_name=list(datasets["dataset_name"])
#         )
# )
# output_files.append(
#     expand(
#         outputDir("tmp/annotate/{dataset_name}_annotate.vcf.gz.tbi", 
#         dataset_name=list(datasets["dataset_name"])
#         )
# )

# Include output for rule 'normalize'
output_files.append(
    expand(
        outputDir("tmp/normalize/{dataset_name}_normalize.vcf.gz"), 
        dataset_name=list(datasets["dataset_name"])
        )
)
output_files.append(
    expand(
        outputDir("tmp/normalize/{dataset_name}_normalize.vcf.gz.tbi"), 
        dataset_name=list(datasets["dataset_name"])
        )
)


# Include output for rule 'filter'
output_files.append(
    expand(
        outputDir("tmp/filter/{dataset_name}_filter.vcf.gz"), 
        dataset_name=list(datasets["dataset_name"])
        )
)
output_files.append(
    expand(
        outputDir("tmp/filter/{dataset_name}_filter.vcf.gz.tbi"), 
        dataset_name=list(datasets["dataset_name"])
        )
)

# [EACH] add qaulity-control snapshot reports
# output_files.append(
#     expand(
#         outputDir("PREP/{dataset_name}/{operation}/", 
#         dataset_name=list(datasets["dataset_name"]),
#         operation=["normalize", "sort", "sort", "filter", "annotate"]
#         )
# )


# Here, we use a `Pandas` `MultiIndex` to group our datasets by ["reference_genome", "dataset_name"]. This allows
# us to iterate over both the `reference_genome` columns unique values AND the `dataset_name` columns and access
# their unique values.
#
# > Think of `MultiIndexes` as an Excel Sheet with the header row and the index columns (Can have more than one)
# > are fixed. Now take that, but add a Z-dimension to the data in the middle that can stills croll around. It 
# > now has levels and you go up or down however many levels there are and you get a whole new set of values for 
# > the colunms and row indexes.
# > 
# > This way of working with data helps us select our data in an intuitive way that also filters it by nature, 
# > since to select the data, you simply need to be able to describe what data you want. This is better for code 
# > compatability as it relies less heavily on query method implementation, and more on data science principles.
#
# TODO: Evaluate porting metadata to MultiIndexes where appropriate.
for reference_genome, groupby_subset in datasets.set_index(["reference_genome", "file"]).groupby(level=0): # [FOR] all unqiue (dataset_name and reference_genome) column combinations present
    if reference_genome != "GRCh38" and groupby_subset is not None: # [IF] reference genome version
        for dataset_name in groupby_subset.index.get_level_values(0): # [FOR] the column in our MultiIndex that contains the dataset_name's in this subset
            output_files.append(
                # [EACH] add liftover request for the DAG
                outputDir("PREP/{dataset_name}/liftover.vcf.gz").format(dataset_name)
            )
            output_files.append(
                # [EACH]{QC} add liftover snapshot
                outputDir("PREP/{dataset_name}/liftover.pdf").format(dataset_name)
            )
rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        [item for sublist in output_files for item in sublist],