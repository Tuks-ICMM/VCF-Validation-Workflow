from snakemake.utils import min_version
from pandas import read_csv
from os.path import join, isfile

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# ENFORCE SNAKEMAKE VERSION CHECK:
min_version("7")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# IMPORT DATASET METADATA:
datasets = read_csv(join("input", "datasets.csv"), header=0)


# IMPORT COMMON FUNCTIONS:
include: "rules/common.py"

# SET GLOBAL DOCKER RECIPIE TO USE:
container: "docker://graemeford/pipeline-os"

rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        expand("results/PREP/{dataset}.vcf.gz", dataset=list(datasets["dataset_name"])),
        expand(
            "results/LIFTOVER/{dataset}.vcf.gz", dataset=list(datasets["dataset_name"])
        ),

# group: VALIDATE
rule strip_INFO_tags:
    """
    Strip out INFO tags
    """
    group: "VALIDATE"
    input:
        lambda wildcards: datasets.loc[
            datasets["dataset_name"] == wildcards.dataset, "file"
        ].item(),
    output:
        pipe("results/PREP/{dataset}/strip_INFO_tags.vcf.gz")
    benchmark:
        "results/_benchmarks/VALIDATE/strip_INFO_tags/{dataset}.tsv"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        echo -e "\n--- LOG SECTION START | BcfTools 'annotate' ---" 1>&2
        bcftools annotate -x INFO,FORMAT -O z -o {output} {input}
        echo -e "--- LOG SECTION END | BcfTools 'annotate' ---\n" 1>&2
        """

# group: VALIDATE
rule FixVcfHeader:
    """
    Validate the VCF header and specification has been followed
    """
    group: "VALIDATE"
    input:
        "results/PREP/{dataset}/strip_INFO_tags.vcf.gz"
    output:
        pipe("results/PREP/{dataset}/FixVcfHeader.vcf.gz")
    benchmark:
        "results/_benchmarks/VALIDATE/FixVcfHeader/{dataset}.tsv"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["piccard"],
    params:
        memory=search("memory", "VALIDATE")
    shell:
        """
        echo -e "--- LOG SECTION START | GATK 'FixVcfHeader' ---" 1>&2
        picard-tools -Xmx{params.memory} FixVcfHeader I=results/PREP/{wildcards.sample}/NO_INFO.vcf.gz O={output}
        echo -e "--- LOG SECTION END | GATK 'FixVcfHeader' ---\n" 1>&2
        """

# group: VALIDATE
rule bcftools_filter:
    """
    Remove variant types we cant yet analyse
    """
    group: "VALIDATE"
    input:
        "results/PREP/{dataset}/FixVcfHeader.vcf.gz"
    output:
        pipe("results/PREP/{dataset}/bcftools_filter.vcf.gz")
    benchmark:
        "results/_benchmarks/VALIDATE/bcftools_filter/{dataset}.tsv"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        memory=search("memory", "VALIDATE")
    shell:
        """
        echo -e "--- LOG SECTION START | BcfTools 'view(SNP,INDEL,exclude(MIXED,MNP,SYMBOLIC,filtered)) ' ---" 1>&2
        bcftools view -v snps,indels -f PASS -O z -o results/PREP/{wildcards.sample}/CLEANED.vcf.gz results/PREP/{wildcards.sample}/NEW_INFO.vcf.gz
        echo -e "--- LOG SECTION END | BcfTools 'view(SNP,INDEL,exclude(MIXED,MNP,SYMBOLIC,filtered)) ---\n" 1>&2
        """

# group: VALIDATE
# conditional: true
if isfile("input/samples.csv"):
    rule bcftools_subset:
        """
        Subset samples according to user defined list and remove variants that do not pass QC.
        """
        group: "VALIDATE"
        input:
            "input/samples.csv",
            "results/PREP/{dataset}/bcftools_filter.vcf.gz"
        output:
            pipe("results/PREP/{dataset}/bcftools_subset.vcf.gz")
        benchmark:
            "results/_benchmarks/VALIDATE/bcftools_subset/{dataset}.tsv"
        resources:
            cpus=search("cores", "VALIDATE"),
            nodes=search("nodes", "VALIDATE"),
            queue=search("queue", "VALIDATE"),
            walltime=search("walltime", "VALIDATE")
        envmodules:
            config["environment"]["envmodules"]["bcftools"],
        params:
            samples=lambda wildcards: samples["dataset" == wildcards.sample]["sample_name"].join(",")
        shell:
            """
            echo -e "--- LOG SECTION START | BcfTools 'view(subset_if_applicable,QC=PASS)' ---" 1>&2
            bcftools view -s {{ params.samples }} -O z -o results/PREP/{wildcards.sample}/FILTERED.vcf.gz results/PREP/{wildcards.sample}/CLEANED.vcf.gz
            echo -e "--- LOG SECTION END | BcfTools 'view(subset_if_applicable,QC=PASS)' ---\n" 1>&2
            """
 
# group: VALIDATE
rule bcftools_normalize:
    """
    Normalise variants - split multiallelic records, left align variants, and ensure variant parsimony
    """
    input:
        lambda wildcards: "results/PREP/{dataset}/bcftools_subset.vcf.gz" if isfile("input/samples.csv") else "results/PREP/{dataset}/bcftools_filter.vcf.gz"
    output:
        pipe("results/PREP/{dataset}/bcftools_normalize.vcf.gz")
    group: "VALIDATE"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    benchmark:
        "results/_benchmarks/VALIDATE/bcftools_normalize/{dataset}.tsv"
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        echo -e "--- LOG SECTION START | BcfTools 'norm' ---" 1>&2
        bcftools norm -m -any results/PREP/{wildcards.sample}/FILTERED.vcf.gz -O z -o results/PREP/{wildcards.sample}/NORMALISED.vcf.gz
        echo -e "--- LOG SECTION END | BcfTools 'norm' ---\n" 1>&2
        """

# group: VALIDATE
rule bcftools_annotate:
    """
    Ensure consistency of chromosome nomenclature for each dataset and sort by chromosome
    """
    input:
        "results/PREP/{dataset}/strip_INFO_tags.vcf.gz"
    output:
        pipe("results/PREP/{dataset}/bcftools_filter.vcf.gz")
    group: "VALIDATE"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    benchmark:
        "results/_benchmarks/VALIDATE/bcftools_annotate/{dataset}.tsv"
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        echo -e "--- LOG SECTION START | BcfTools 'annotate' ---" 1>&2
        bcftools annotate --rename-chrs input/rename_chr.txt results/PREP/{wildcards.sample}/NORMALISED.vcf.gz | bcftools sort -m {params.memory} -T results/PREP/{wildcards.sample} -O z -o results/PREP/{wildcards.sample}.vcf.gz
        echo -e "--- LOG SECTION END | BcfTools 'annotate' --- \n" 1>&2
        """

# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    input:
        "results/{operation}/{dataset}/{output}.vcf"
    output:
        "results/{operation}/{dataset}/{output}.vcf.gz.tbi",
    group: "VALIDATE"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    benchmark:
        "results/_benchmarks/{operation}/tabix/{dataset}_{output}.log"
    shell:
        """
        cd results/{operation}/{dataset}/
        tabix -p vcf {input}
        """


rule LIFTOVER:
    """
    Lift Variants onto same Reference build. Otherwise we cant merge them or analyse them in context of each other.
    """
    log:
        "logs/LIFTOVER/{dataset}.log",
    input:
        "results/PREP/{dataset}.vcf.gz",
        "results/PREP/{dataset}.vcf.gz.tbi",
    output:
        "results/LIFTOVER/{dataset}.vcf.gz",
    group: "LIFTOVER"
    benchmark:
        "results/_benchmarks/LIFTOVER/{dataset}.log"
    params:
        prefix=lambda wildcards: "results/LIFTOVER/{dataset}_LIFTED".format(
            dataset=wildcards.dataset
        ),
        exclusionList=(
            lambda wildcards: "results/LIFTOVER/{dataset}_EXCLUDE.dat".format(
                dataset=wildcards.dataset
            )
        ),
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        LiftOver=join("resources", "liftOverPlink.py"),
        rmBadLifts=join("resources", "rmBadLifts.py"),
        ref=join(
            *next(
                i["file_path"]
                for i in config["reference-genomes"]
                if i["version"] == "GRCh38"
            ),
        ),
        mem=f'-Xmx{search("memory", "LIFTOVER")} ',
    resources:
        cpus=search("cores", "LIFTOVER"),
        nodes=search("nodes", "LIFTOVER"),
        queue=search("queue", "LIFTOVER"),
        walltime=search("walltime", "LIFTOVER"),
    script:
        join("scripts", "00-LIFTOVER.py")
